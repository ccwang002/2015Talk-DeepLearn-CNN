<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>CNN From the Ground Up, Liang2</title>
  <meta name="viewport" content="width=792, user-scalable=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  <!-- Icon -->
  <link href="pics/favicon.png" rel="icon" type="image/x-icon" />
  <!-- MathJax -->
  <!-- CSS Stle -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.css" integrity="sha256-kAbX9Z5ZMhjo34PRGEMc7RiueevlbmDilvUt1NsZGAI=" crossorigin="anonymous" />
  <link rel="stylesheet" href="lib/shower/themes/ribbon/styles/screen.css">
  <link rel="stylesheet" href="lib/highlight/styles/tomorrow.css" type="text/css"/>
  <link rel="stylesheet" href="static/custom.css" type="text/css"/>
</head>
<body class="list">
  <!-- Header in overview -->
  <header class="caption">
    <h1>Convolutional Neural Network From the Ground Up</h1>
    <p style="line-height: 32px; padding-top:15px;"><a href="http://liang2.tw">Liang Bo Wang (亮亮)</a>, 2015-05-18</p>
  </header>

  <!-- START OF SLIDE CONTENT -->

  <!-- Cover slide -->
  <section id="cover" class="slide cover w"><div>
    <h3 id="talk-subheader">Taiwan R User Group, 2015-05-18</h3>
    <h2 id="talk-header" class="place">Convolutional Neural Network (CNN) From the Ground Up</h2>
    <p id="talk-orig-author">
      Orig. <a href="http://cs231n.stanford.edu/">Stanford CS231n course</a> by Andrej Karpathy and <br> Li Fei-Fei under MIT License
    </p>
    <p id="talk-author">
      Adapted by <a href="http://liang2.tw" target="_blank">Liang<sup>2</sup></a> under CC 4.0 BY license
    </p>
    <p id="usage-instr">
      <kbd>Esc</kbd> to overview <br />
      <kbd>←</kbd> <kbd>→</kbd> to navigate
    </p>
    <img src="pics/cover.jpg" alt="">
  </div>
  <style>
    #talk-header {
      color: #D91A3C;
      text-align: center;
      font-size: 80px;
      line-height: 1.2em;
      opacity: 0.8;
      position: relative;
      top: 20px;
      width: 120%;
    }
    #talk-subheader {
      color: #0376A6;
      text-align: center;
      font-size: 32px;
      opacity: 0.3;
      position: relative;
      top: -90px;
    }
    #talk-orig-author,
    #talk-author {
      position: relative;
      font-size: 24px;
      line-height: 1.2em;
      text-shadow: 1px 1px 3px #000;
    }
    #talk-orig-author {
      top: -60px;
    }
    #talk-author {
      top: -50px;
    }
    #talk-orig-author a,
    #talk-author a {
      color: #FFFA20;
    }
    #cover p {
      margin: 10px 0 0;
      text-align: center;
      color: #FFF;
      font-size: 32px;
      opacity: 0.8;
    }
    #usage-instr {
      position: absolute;
      text-align: right;
      right: 30px;
      bottom: 20px;
    }
    #usage-instr kbd {
      opacity: 0.8;
      color: #D91A3C;
      background-color: white;
    }
    #cover .src-link {
      position: absolute;
      font-size: 14px;
      text-align: right;
      bottom: 10px;
      right: 10px;
    }
    #cover img {
      opacity: 1;
    }
  </style>
  </section>

  <section id="disclaimer" class="slide w shout"><div>
      <h2>I don't own / create most of the content</h2>
    </div>
    <style>
      #disclaimer h2 {
        color: #D91A3C;
      }
    </style>
  </section>

  <section id="disclaimer-again" class="slide w shout"><div>
      <h2>Almost everything are from <br><a href="http://cs231n.stanford.edu/">Stanford CS231n</a></h2>
    </div>
    <style>
      #disclaimer-again h2 {
        color: purple;
      }
      #disclaimer-again h2 a {
        color: purple;
      }
    </style>
  </section>

  <section class="slide"><div>
      <figure>
        <blockquote>
          <p>Education is the art of conveying a sense of truth by telling a series of decreasing lies.</p>
        </blockquote>
        <figcaption>Steven Wittens, creator of MathBox</figcaption>
      </figure>
      <p>Far more details about DNN (CNN) are left out of this talk due to the time constraint. However, you should get the idea how to implement the core parts and be able to read the rest of the materials yourself.</p>
  </div></section>

  <section id="about-me" class="slide"><div>
      <h2>About Me</h2>
      <ul>
        <li>Master student at <br />
          Bioinfo &amp; Biostat Core Lab, NTU CGM</li>
        <li>R / Python</li>
        <li>Taiwan R co-organizer</li>
        <li>PyCon APAC 2014-15/TW staff</li>
        <li>Former intern at Microsoft Research Asia</li>
        <li>Happy to chat about Bioinfo <del>and anime</del></li>
      </ul>
      <img id="protrait" src="pics/me.jpg" class="place r"alt="">
    </div>
    <style>
      #protrait {
        margin-right: 120px;
        width: 300px;
      }
    </style>
  </section>

  <section id="super-img-class" class="slide twocol"><div>
      <h2>Supervised Image Classification</h2>
      <div class="left">
        <ul>
          <li><strong>Input:</strong> RGB pixel values</li>
          <li><strong>Output:</strong> classes / labels, usually with a score</li>
          <li>The classifier learns their relationship from data</li>
          <li>So data are splitted into training / validation / testing datset</li>
        </ul>
      </div>
      <div id="cite-lovelive" class="place b r citation">
        <img src="pics/external/lovelive_classification.jpg" alt="">
        <p>Ref: <a href="http://christina.hatenablog.com/entry/2015/01/23/212541">Deep Learningでラブライブ！キャラを識別する</a></p>
      </div>
      <style>
        #super-img-class .left {
          width: 40%;
        }
        #cite-lovelive {
          margin-bottom: 50px;
          width: 420px;
        }
      </style>
  </div></section>

  <section id="end-to-end" class="slide"><div>
      <h2>CNN / DL are end-to-end classification</h2>
      <div class="citation">
        <img src="pics/external/lenet.png" alt="">
        <p>Ref: <a href="http://deeplearning.net/tutorial/lenet.html">Deeplearning.net LeNet tutorial</a></p>
      </div>
      <ul>
        <li>The CNN idea exists for 15+ years [LeCun 1998]</li>
        <li>A feeding frenzy in recent 5 fives thanks to tons of news coverage and GPU computation</li>
        <li>We will implement it with our bare hands today (though skipping <em>a lot</em>)</li>
      </ul>
  </div></section>

  <section id="general-ml-framework" class="slide"><div>
      <h2>General machine learning framework</h2>
      <ol>
        <li>A mapping function \(f\) from input features \(x\) to scores of output class \(f\).
          Class with highest score is chosed as the prediction for input \(i\).
          $$
          \begin{aligned}
          \hat{y_i} = \arg\max_j f(x_i; W)_j \\
          j, y_i \in \{1 \ldots K\}
          \end{aligned}
          $$</li>
        <li>A loss function \(L\) to compute the difference between \(y_i\) and \(\hat{y_i}\).</li>
        <li>Optimize (Lower) the loss by updating the parameter \(W\).</li>
        <li>Stop the update when max iterations are reached or stop criteria meets.</li>
      </ol>
  </div></section>

  <section id="linear-class-cover" class="slide cover w subheader" ><div>
      <h2>Softmax Linear Classification</h2>
      <img src="pics/mid.jpg" alt="">
    </div>
    <style>
      #linear-class-cover h2 {
      }
    </style>
  </section>

  <section class="slide"><div>
      <h2>Linear classifier</h2>
       <p>Training dataset of images \(x_i \in R^D\) each with label \(y_i\) for \(N\) examples, \(i = 1 \ldots N\) and \(y_i \in 1 \ldots K\).

        $$f(x_i; W, b) = W x_i + b \quad\quad f: R^D \mapsto R^K$$

        where parameters are \(W\) [K x D] called <strong>weights</strong> and \(b\) [K x 1] called <strong>bias vector</strong>.
       </p>
      <p>In CIFAR-10 dataset, \(N\) = 50,000, \(D\) = 32 x 32 x 3 = 3072, \(K\) = 10.</p>
  </div></section>

  <section class="slide"><div>
      <h2>Linear classifier visualized</h2>
      <div class="citation">
        <img src="pics/external/cs231n_note_linear_classification.png" alt="">
        <p>Ref: <a href="https://cs231n.github.io/linear-classify/">CS231n Note: Linear Clasification</a></p>
      </div>
  </div></section>

  <section class="slide"><div>
      <h2>Bias trick</h2>
      <div class="citation">
        <img src="pics/external/cs231n_note_bias_trick.png" alt="">
        <p>Ref: <a href="https://cs231n.github.io/linear-classify/">CS231n Note: Loss optimization</a></p>
      </div>
  </div></section>

  <section class="slide"><div>
      <h2>Loss function - Softmax</h2>
      <p>
        $$
        L_i = -\log\left(\frac{e^{f_{y_i}}}{\sum_j e^{f_j}}\right)
        $$
        \(e^{f_{y_i}}\) is like probability yet being normalized. Ex. \(-\log 10 &lt; -\log 0.01 \). Higher score leads to lower loss. <br>
        For numerical stability, in computation \(
        \frac{e^{f_{y_i}}}{\sum_j e^{f_j}}
        = \frac{C \cdot e^{f_{y_i}}}{C \cdot \sum_j e^{f_j}}
        = \frac{e^{f_{y_i} + \log C}}{\sum_j e^{f_j + \log C}}
        \) and let \(
          \log C = -\max_j f_j
        \) to normalized the exp. of the largest score to be 1.</p>
  </div></section>

  <section class="slide"><div>
      <h2>L2 Regularization</h2>
      <ul>
        <li>Set of parameters \(W\) having the lowest loss is not unique. \(\phi W, \phi &gt; 1\)</li>
        <li>Regularization of \(L2\) norm and a hyperparameter \(\lambda\) to tune the strength
          $$
            R(W) = \sum_k\sum_l W_{k,l}^2
          $$
        </li>
        <li>Full loss function includes avg. data loss and regularization loss
          $$
          L =
            \underbrace{ \frac{1}{N} \sum_i L_i }_\text{data loss} +
            \underbrace{ \lambda R(W) }_\text{regularization loss}
          $$
        </li>
      </ul>
  </div></section>

  <section class="slide"><div>
      <h2>Regularization (cont'd)</h2>
      <ul>
        <li>Penalizing large weights tends to improve generalization.</li>
        <li>For example, $$ x = [1, 1, 1, 1] \quad w_1 = [1, 0, 0, 0] \quad w_2 = [0.5, 0.5, 0.5, 0.5]$$
        where their L2 penalty are 0.5 and 0.25 respectively, \(w_2\) is preferred.</li>
        <li>L2 penalty prefers smaller and more diffuse weight vectors. Imply using more input dimensions instead of betting on one or two.</li>
      </ul>
  </div></section>

  <section class="slide"><div>
      <h2>Loss optimization</h2>
      <ul>
        <li>How to update the parameters based on this loss function?</li>
        <li>It's like being lost in a mountain and try to find a way home, no map.</li>
        <li>Height analogs to our loss function; position is our 2D parameters.</li>
        <li>Strategy: <strong>gradients of the loss function</strong>. <br>
        Feel feet on your ground, follow the slope of hills.
          $$
            \frac{df(x)}{dx} = \lim_{h\ \to 0} \frac{f(x + h) - f(x)}{h}
          $$
        </li>
        <li>Aka numerical gradients. Practically use \([f(x + h) -  f(x - h)] / 2h\)</li>
      </ul>
  </div></section>

  <section class="slide"><div>
      <h2>Choose the step size (learning rate)</h2>
      <div class="citation">
        <img src="pics/external/cs231n_note_vis_loss_fun.png" alt="">
        <p>Ref: <a href="https://cs231n.github.io/optimization-1/">CS231n Note: Linear Clasification</a></p>
      </div>
  </div></section>

  <section class="slide"><div>
      <h2>Analytical gradients</h2>
      <ul>
        <li>Numerical gradients is slow (think of computing this for all \(W\) dimensions.</li>
        <li>We know Calculus ... so what's the gradient of softmax?
          $$
          \frac{\partial L_i}{ \partial f_j} = - \left[\mathbb{1}(i = j) - p_j\right] \quad
            p_i = \frac{e^{f_i}}{\sum e^{f}} \quad
            \frac{\partial f_j}{ \partial w_j} = x_j
          $$
        </li>
        <li>Use the chain rule to get the gradient for
          \(\frac{\partial L_i}{ \partial w_j}
          = \frac{\partial L_i}{ \partial f_j} \cdot \frac{\partial f_j}{ \partial w_j}\)
        </li>
        <li>Already on edge? Check with the numerical gradients.</li>
      </ul>
  </div></section>

  <section class="slide"><div>
      <h2>Gradient Descent</h2>
      <pre class="language-python tighter">
      <code># Vanilla Minibatch Gradient Descent
while it &lt; max_iter:
    data_batch = sampling(data, 256)
    dw = gradient(loss_fun, data_batch, w)
    w += - step_size * w</code>
      </pre>
    <p class="tighter">Stochastic Gradient Descent (SGD) when batch contains only 1 sample.</p>
  </div></section>

  <section class="slide"><div>
      <h2>Get our hands wet</h2>
      <p>For what does the CIFAR-10 dataset look like and data preprocessing, try <a href="http://nbviewer.ipython.org/github/ccwang002/2015Talk-DeepLearn-CNN/blob/master/code/00_cifar10.ipynb">ænotebook 00</a>.</p>
      <p>For softmax linear classifier, try <a href="http://nbviewer.ipython.org/github/ccwang002/2015Talk-DeepLearn-CNN/blob/master/code/01_softmax.ipynb">notebook 01</a>.</p>
  </div></section>

  <section id="nn-cover" class="slide cover w subheader"><div>
      <h2>Fully Connected<br>Neural Network</h2>
      <img src="pics/mid.jpg" alt="">
    </div>
  </section>

  <section class="slide"><div>
      <h2>Our "artifact" neuron as a linear classifier</h2>
      <div class="citation">
        <img src="pics/external/cs231n_note_neuron.png" alt="" />
        <p>Ref: <a href="https://cs231n.github.io/neural-networks-1/">CS231n Note: Neural Network 1</a></p>
      </div>
      <p>We model the neuron by two parts: linear combination \(\sum w_i x_i\) and activation function \(f\)</p>
  </div></section>

  <section class="slide"><div>
      <h2>Choices for activation function</h2>
      <ul>
        <li>Traditionally, sigmoid \(\sigma(x) = 1 / (1 + e^{-x})\) and tanh.</li>
        <li>We use <strong>ReLU</strong> (The Rectified Linear Unit): $$f(x) = max(0, x)$$</li>
        <li>There remains many alternatives, such as leaky ReLU and maxout.</li>
        <li>Activation function introduces the non-linearity.</li>
      </ul>
  </div></section>

  <section class="slide"><div>
      <h2 class="tighter">Fully-connected layer</h2>
      <div class="citation">
        <img src="pics/external/cs231n_note_nn_arch.png" alt="">
        <p>Ref: <a href="https://cs231n.github.io/neural-networks-1/">CS231n Note: Neural Network 1</a></p>
      </div>
      <p>Total size of parameters (weights and biases) [input x #neuron]:<br>
        Left is (3 + 1) x 4 + (4 + 1) x 2 = 26.<br>
        Right is (3 + 1) x 4  + (4 + 1) x 4 + (4 + 1) x 1 = 41.</li>
      </p>
  </div></section>

  <section class="slide"><div>
      <h2>Example feed-forward computation</h2>
      <pre class="language-python tighter">
<code># forward-pass of a 3-layer neural network:
f = lambda x: np.where(x &gt; 0, x, 0)  # ReLU activation
x = np.random.randn(3, 1)    # input vector (3x1)
h1 = f(np.dot(W1, x) + b1)   # 1st hidden layer (4x1)
h2 = f(np.dot(W2, h1) + b2)  # 2nd hidden layer (4x1)
out = np.dot(W3, h2) + b3    # output neuron (1x1)</code>
      </pre>
      <p>The dim of Ws, bs: W1(4x3), b1(4x1), W2(4x4), b2(4x1), W3(4x1), b3(1x1)<br>
        Note that x can be (3xN) for batch input; no act. fun. in output layer.</p>
  </div></section>

  <section id="back-prop" class="slide"><div>
      <h2 class="tighter">Backward propagation</h2>
      <ul>
        <li>Response of output layer, \(out\), goes to the loss function.</li>
        <li>Loss function returns the update direction for response, \(\Delta out\).</li>
        <li>But how to modify \(W\)s and \(b\)s to yield the desired change of \(\Delta out\)?</li>
        <li>Recall Calculus ... <strong>Use chain rule</strong>
          $$
          \begin{aligned}
          f(x,y) = x y \quad &\rightarrow \quad \frac{\partial f}{\partial x} = y  \quad \frac{\partial f}{\partial y} = x \\
          f(x,y) = x + y \quad &\rightarrow \quad \frac{\partial f}{\partial x} = 1 \quad \frac{\partial f}{\partial y} = 1 \\
          f(x,y) = \max(x, y) \quad &\rightarrow \quad \frac{\partial f}{\partial x} = \mathbb{1}(x >= y) \quad \frac{\partial f}{\partial y} = \mathbb{1}(y >= x)
          \end{aligned}
          $$
        </li>
      </ul>
  </div></section>

  <section id="back-prop-first-try" class="slide twocol"><div>
      <div class="left">
        <p>
          $$
          \begin{aligned}
          f(x, y, z) = (x + y) z \\
          q = x + y, \enspace f = qz
          \end{aligned}
          $$
          $$
          \begin{aligned}
          \Rightarrow \frac{\partial q}{\partial x}
            &= \frac{\partial q}{\partial y} = 1
          \end{aligned}
          $$

          $$
          \begin{aligned}
          \Rightarrow \frac{\partial f}{\partial x}
            &= \frac{\partial f}{\partial q}\frac{\partial q}{\partial x} = z \cdot 1\\
          \frac{\partial f}{\partial y}
            &= \frac{\partial f}{\partial q}\frac{\partial q}{\partial y} = z \cdot 1\\
          \frac{\partial f}{\partial z}
            &= q
          \end{aligned}
          $$
        </p>
      </div>
      <div class="right">
        <pre class="language-python small">
<code># set some inputs
x = -2; y = 5; z = -4

# perform the forward pass
q = x + y  # q = 3
f = q * z  # f becomes -12

# perform the backward pass (back prop)
# in reverse order:
# first backprop through f = q * z
dfdz = q  # = 3
dfdq = z  # = -4
# now backprop through q = x + y
dfdx = 1.0 * dfdq  # = -4, dq/dx = 1
dfdy = 1.0 * dfdq  # = -4, dq/dy = 1</code>
        </pre>
        <div class="citation next place r b">
          <div class="placeholder"></div>
        <img src="pics/external/cs231n_note_back_prop_simple.png" alt="">
        <p>Ref: <a href="https://cs231n.github.io/optimization-2/">CS231n Note: Optimization 2</a></p>
        </div>
      </div>
      <style>
        #back-prop-first-try .citation {
          width: 40%;
          margin-bottom: 40px;
        }
      </style>
  </div></section>

  <section id="back-prop-ex" class="slide"><div>
      <h2 class="tighter">Backward propagation example</h2>
      <p>Think of a sigmoid function,
        \(
        f(w,x) = \frac{1}{1+e^{-(w_0x_0 + w_1x_1 + w_2)}}
        \).<br>
        First compute all the building blocks,
        $$
        \begin{aligned}
        f(x) = \frac{1}{x}
        \quad &\rightarrow \quad
        \frac{df}{dx} = -1/x^2
        \\
        f_c(x) = c + x
        \quad &\rightarrow \quad
        \frac{df}{dx} = 1
        \\
        f(x) = e^x
        \quad &\rightarrow \quad
        \frac{df}{dx} = e^x = f(x)
        \\
        f_a(x) = ax
        \quad &\rightarrow \quad
        \frac{df}{dx} = a
        \end{aligned}
        $$
        Given initial w0, x0, w1, x1, w2 values (cont'd on next page)
      </p>
  </div></section>

  <section id="back-prop-vis" class="slide"><div>
      <h2>Backward propagation example (cont'd)</h2>
      <div class="citation">
        <img src="pics/external/cs231n_note_back_prop.png" alt="">
        <p>Ref: <a href="https://cs231n.github.io/optimization-2/">CS231n Note: Optimization 2</a></p>
      </div>
  </div></section>

  <section class="slide"><div>
      <h2>Backprop in practice</h2>
      <ul>
        <li>Compute all backprop gradients. Ex. Compute both dW and dx.</li>
        <li>Cache forward pass variables. Ex. Cache all W, x</li>
        <li>Since it is chain rule, always multiply the gradients from upper stage.</li>
        <li>Common gates (acts like): add (pass), max (switch) and multiply (swap).</li>
        <li>How to expand to matrix-matrix gradient computation?</li>
        <li>Dimension of x and dx <strong>match</strong>.<D-r></li>
      </ul>
  </div></section>

  <section class="slide"><div>
      <h2>Matrix-Matrix multiply gradient</h2>
      <pre class="language-python">
<code># forward pass
W = np.random.randn(5, 10)
X = np.random.randn(10, 3)
D = W.dot(X)  # shape (5, 3)
# ... suppose we had the gradient on D from upper stage
dD = np.random.randn(*D.shape)  # same shape as D
dW = dD.dot(X.T)  # X.T = X's transpose matrix
dX = W.T.dot(dD)</code></pre>
  </div></section>

  <section class="slide"><div>
      <h2>Get our hands wet</h2>
      <p>Now we are able to make a 2-layer fully connected (FC) NN, which has the structure:
        <span class="blue">input - FC layer - ReLU - FC layer - softmax - out class</span></p>
      <p>It's in <a href="http://nbviewer.ipython.org/github/ccwang002/2015Talk-DeepLearn-CNN/blob/master/code/02_two_layer_net.ipynb">notebook 02</a>.</p>
      <p>But I'm afraid we are almost running out of time so probably just see the final result to get the whole picture.</p>
  </div></section>

  <section class="slide"><div>
      <h2>Before learning: sanity checks tips/tricks</h2>
      <ul>
        <li>Gradient check using numerical gradients.</li>
        <li>Turn off regularization to check for correct loss at chance performance. Ex. CIFAR-10 initial softmax loss is around 2.302 (= -ln 0.1).</li>
        <li>Increase regularization should increase loss.</li>
        <li>Overfit a tiny subset of data (e.g. 20 samples).</li>
      </ul>
  </div></section>

  <section class="slide"><div>
      <h2>Back on parameter updates</h2>
      <ul>
        <li>Previously we use SGD and an analogue of finding path in mountains to home by approaching the inverse direction of slope.</li>
        <li>In real world, <strong>the slope controls the acceleration rate</strong> not the speed itself.</li>
        <li>Learning rate decay.</li>
        <li>Momentum update: \(mu \in [0, 1)\)</li>
      </ul>
      <pre class="language-python">
<code>v = mu * v - learning_rate * <mark class="warning">dx</mark> # integrate velocity
<mark class="warning">x</mark> += v # integrate position</code>
          </pre>
  </div></section>

  <section id="comp-param-update" class="slide"><div>
      <h2 class="tighter">Parameter update method comparison</h2>
      <div class="citation">
        <div class="twocol">
          <img src="pics/external/cs231n_note_opt1.gif" alt="">
          <img src="pics/external/cs231n_note_opt2.gif" alt="">
        </div>
        <p class="note">Animations that may help your intuitions about the learning process dynamics. <strong>Left</strong>: Contours of a loss surface and time evolution of different optimization algorithms. Notice the "overshooting" behavior of momentum-based methods, which make the optimization look like a ball rolling down the hill. <strong>Right</strong>: A visualization of a saddle point in the optimization landscape, where the curvature along different dimension has different signs (one dimension curves up and another down). Notice that SGD has a very hard time breaking symmetry and gets stuck on the top. Conversely, algorithms such as RMSprop will see very low gradients in the saddle direction. Tue to the denominator term in the RMSprop update, this will increase the effective learning rate along this direction, helping RMSProp proceed.
        </p>
        <p>Ref: <a href="https://cs231n.github.io/neural-networks-3/">CS231n Note: Neural Network 3</a> and Image credit: <a href="https://twitter.com/alecrad">Alec Radford</a>.</p>
      </div>
      <style>
        #comp-param-update .citation .twocol {
          text-align: center;
        }
        #comp-param-update .citation .twocol > img:nth-child(1) {
          margin-left: auto;
          width: 47%;
        }
        #comp-param-update .citation .twocol> img:nth-child(2) {
          width: 47%;
        }
        #comp-param-update .citation > p:not(.note) {
          margin-top: 10px;
          margin-right: 15px;
        }
        #comp-param-update .citation > p.note {
          margin-top: 0px;
          margin-right: 15px;
          margin-left: 15px;
          text-align: left;
          font-size: 14px;
          line-height: 18px;
          color: #888;
        }

      </style>
  </div></section>

  <section class="cnn-cover slide cover w subheader"><div>
      <h2>Convolutional Neural Network (CNN)</h2>
      <img src="pics/mid.jpg" alt="">
  </div></section>

  <section class="slide"><div>
      <h2>ConvNet introduces depth</h2>
      <div class="citation">
        <img src="pics/external/cs231n_note_cnn_arch.png" alt="">
        <p>Ref: <a href="https://cs231n.github.io/convolutional-networks/">CS231n Note: Convolutional Netowrks</a></p>
      </div>
      <p>Input viewed as 32 width x 32 height x 3 channels (depth)<br>
        Output viewed as 1 x 1 x 10. Spatial information is preserved.</p>
  </div></section>

  <section class="slide"><div>
      <div class="citation">
        <img src="pics/external/cs231n_note_receptive_field.png" alt="">
        <p>Ref: <a href="https://cs231n.github.io/convolutional-networks/">CS231n Note: Convolutional Netowrks</a></p>
      </div>
      <p>Receptive field: height and width of the spatially bounded input of <em>all depths</em> the neuron next stage takes from the former layer.</p>
  </div></section>

  <section class="slide"><div>
      <h2>Convolutional layer - spatial arrangement</h2>
      <ul>
        <li><strong>Depth:</strong> multiple neurons receiving signals from the same receptive field forms a depth column</li>
        <li><strong>Stride:</strong> overlapping between adjacent receptive fields</li>
        <li><strong>Zero-padding:</strong> to maintain the W x H dimension of next layer, pad the input with zeros on the border of the input volume</li>
        <li>Given input volume size \(W\), receptive field size \(F\), stride size \(S\) and amount of zero padding \(P\), the output dimension is $$ (W-F + 2P) / S + 1 $$</li>
      </ul>
  </div></section>

  <section class="slide"><div>
      <h2>Receptive field vs output size example</h2>
      <div class="citation">
        <img src="pics/external/cs231n_note_receptive_field_ex.png" alt="">
        <p>Ref: <a href="https://cs231n.github.io/convolutional-networks/">CS231n Note: Convolutional Netowrks</a></p>
      </div>
  </div></section>

  <section class="slide"><div>
      <h2>Conv. layer - spatial arrangement (cont'd)</h2>
      <ul>
        <li>Use of zero padding: when \(P = (F - 1) / 2\) and \(S=1\), the input volume and output volume have same size.</li>
        <li>Not every stride value is feasible.</li>
        <li>Ex. 2012 ImageNet Krizhevsky et al. architecture uses input size [227x227x3], fist Conv layer uses F = 11, S = 4, P = 0 and depth K = 96. <br>
          The output volume should be (227 - 11 + 2 x 0) / 4 + 1 = 55, [55x55x96]. Each neurons in the Conv layer has receptive field [11x11x3].</li>
      </ul>
  </div></section>

  <section class="slide"><div>
      <h2>Conv. layer - parameter sharing</h2>
      <ul>
        <li>Now we have 55x55x96 = 290,400 neurons in the first Conv layer, and each has 11x11x3 = 363 weights + 1 bias. Together this adds up to 290,400 x 364 = 105,706,600 parameters for the first layer alone.</li>
        <li>Assumption: <strong>neurons at same depth share same parameters</strong></li>
        <li>So now parameters reduce to (363 + 1) x 96 = 34,944 parameters.</li>
        <li>Neurons at same depth use same parameters, referred as <strong>filter</strong> or <strong>kernel</strong></li>
        <li>These 96 filters of first Conv layer can be visualized.</li>
      </ul>
  </div></section>

  <section class="slide"><div>
      <h2>ImageNet 1st Conv layer filter visualization</h2>
      <div class="citation">
        <img src="pics/external/cs231n_note_conv_filter.png" alt="">
        <p>Ref: <a href="https://cs231n.github.io/convolutional-networks/">CS231n Note: Convolutional Netowrks</a></p>
      </div>
  </div></section>

  <section class="slide"><div>
      <h2>Conv. layer - summary</h2>
      <ul>
        <li>Accepts a volume of size \(W_1 \times H_1 \times D_1 \)</li>
        <li>Four hyperparameter: depth(#filters) \(K\), receptive field size \(F\), stride \(S\) and amount of zero padding \(P\)</li>
        <li>Produces a volume of size \(W_2 \times H_2 \times D_2\), where \(W_2 = (W_1 - F + 2P)/S + 1\), and \(D_2 = K\)</li>
        <li>With parameter sharing, it introduces \(F \cdot F \cdot D_1\) weights per filter, thus for a total of \((F \cdot F \cdot D_1) \cdot K\) weights and \(K\) biases.</li>
      </ul>
  </div></section>

  <section class="slide"><div>
      <div class="citation">
        <p style="text-align: left; margin: 0 0 5px 10px;">Ref: <a href="https://cs231n.github.io/convolutional-networks/">CS231n Note: Convolutional Netowrks</a></p>
        <iframe id="embed-frame" src="https://cs231n.github.io/assets/conv-demo/index.html" frameborder="0"></iframe>
      </div>
      <style>
        #embed-frame {
          width: 100%;
          height: 700px;
          -ms-zoom: 0.75;
          -moz-transform: scale(0.75);
          -moz-transform-origin: 0 0;
          -o-transform: scale(0.75);
          -o-transform-origin: 0 0;
          -webkit-transform: scale(0.75);
          -webkit-transform-origin: 0 0;
        }
      </style>
  </div></section>

  <section class="slide"><div>
      <h2>Max Pooling Layer</h2>
      <div class="citation">
        <img src="pics/external/cs231n_note_conv_pooling.png" alt="">
        <p>Ref: <a href="https://cs231n.github.io/convolutional-networks/">CS231n Note: Convolutional Netowrks</a></p>
      </div>
  </div></section>

  <section class="slide"><div>
      <h2>Converting FC layer to CONV layers</h2>
      <p>An FC layer with \(K = 4096\) looking at Conv layer of [7x7x512] volumes, using filter size \(F = 7\) gives output volume [1x1x4096].</p>
      <p>An FC layer looks at the previous layer with \(K = 1000\) and \(F=1\) gives output [1x1x1000].</p>
      <p>That's all you need for a ConvNet :)</p>
  </div></section>

  <section class="slide"><div>
      <h2>Layer Patterns</h2>
      <p class="blue">INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC</p>
      <ul>
        <li>INPUT -> CONV -> RELU -> FC</li>
        <li>INPUT -> [CONV -> RELU -> POOL]*2 -> FC -> RELU -> FC</li>
        <li>INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL]*3 -> [FC -> RELU]*2 -> FC</li>
        <li>Multiple stacked CONV layers can develop more complex features of the input volume before the destructive pooling operation</li>
      </ul>
  </div></section>

  <section class="slide"><div>
      <h2>You should be able to read these CNNs</h2>
      <div class="citation">
        <img src="pics/external/lenet.png" alt="">
        <p>Ref: <a href="http://deeplearning.net/tutorial/lenet.html">Deeplearning.net LeNet tutorial</a></p>
      </div>
      <div class="citation">
        <img src="pics/external/lenet5.png" alt="">
        <p>Ref: <a href="http://eblearn.sourceforge.net/beginner_tutorial2_train.html">EBLearn LeNet-5</a></p>
      </div>
  </div></section>

  <section class="slide"><div>
      <h2>Get our hands wet</h2>
      <p><a href="http://nbviewer.ipython.org/github/ccwang002/2015Talk-DeepLearn-CNN/blob/master/code/03_layers.ipynb">Notebook 03</a> works forward/backward implementation for all types of layer.</p>
      <p><a href="http://nbviewer.ipython.org/github/ccwang002/2015Talk-DeepLearn-CNN/blob/master/code/04_convnet.ipynb">Notebook 04</a> creates a <span class="blue">CONV-RELU-POOL-FC</span> two-layer ConvNet.</p>
  </div></section>

  <section id="what-missed" class="slide twocol"><div>
    <h2>What we've skipped</h2>
    <div class="left">
      <ul>
        <li>kNN classifier</li>
        <li>SVM loss function</li>
        <li>Gradient random local search</li>
        <li>Other neuron activation functions</li>
        <li>Overfitting/generalization tradeoff</li>
        <li>Data preprocessing</li>
      </ul>
    </div>
    <div class="right">
      <ul>
        <li>L1 regularization</li>
        <li>Dropout</li>
        <li>Tips for tuing the learning process</li>
        <li>Ratio of weights:updates</li>
        <li>Other parameter update methods</li>
        <li>Parameter search</li>
        <li>We just scratched the surface of CNN, reread the whole part</li>
      </ul>
    </div>
    <style>
      #what-missed .left {
        width: 45%;
        margin-right:10px;
      }
    </style>
  </div></section>

  <section id="thanks-orig-author" class="slide shout w">
    <h2>If you like the content, go thank the <a href="https://twitter.com/karpathy">original author</a> (not me)</h2>
    <style>
      #thanks-orig-author h2,
      #thanks-orig-author h2 a {
        color: #D91A3C;
      }
    </style>
  </section>

  <!-- End Slide -->
  <section id='end' class='slide cover shout w'><div>
    <h2>Thank You!</h2>
    <img src="pics/end.jpg" alt="">
  </div><style>
    #end h2 {
      position: absolute;
      text-align: left;
      top: 300px;
      left: 50px;
      color: #0376A6;
      font-size: 92px;
      opacity: 0.9;
    }
  </style></section>

  <!-- END OF SLIDE CONTENT -->
  <p class="badge"><a href="https://github.com/ccwang002/2015Talk-DeepLearn-CNN" target="_blank">Fork me on Github</a></p>
  <div class="progress"><div></div></div>

  <!-- Library -->
  <script src="lib/highlight/highlight.pack.js" type="text/javascript" charset="utf-8"></script>
  <script>
    hljs.initHighlightingOnLoad();
  </script>
  <script src="lib/shower/shower.min.js"></script>
  <!-- Mathjax -->
  <!-- During local development, use localhost mathjax for speed-->
  <!--<script src="file:///Users/liang/.ipython/profile_default/static/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->
  <!-- online Mathjax CDN -->
  <!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/katex.min.js" integrity="sha256-Jv5VrSHYxxBmh18hQUviB4OiVzOkDHdO0uY4GtrCMDQ=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.8.3/contrib/auto-render.min.js" integrity="sha256-Wb+blP9KCjsV65XUTmxKT/fisoQQM3D4bqphG8l+p9E=" crossorigin="anonymous"></script>
  <script>
    renderMathInElement(document.body);
  </script>
</body>
</html>
